"""
FastAPI Backend Server for PsychoGraph Instagram Chat Analyzer
This provides a web API that the dashboard can call to analyze files
without needing to run Python scripts manually from command line.

Usage:
  1. pip install fastapi uvicorn anthropic
  2. export ANTHROPIC_API_KEY="your-key-here"
  3. python server.py
  4. Open browser to http://127.0.0.1:8000
"""

import json
import os
import tempfile
import shutil
from typing import Optional
from pathlib import Path

# â”€â”€ FastAPI imports for building the web server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from fastapi import FastAPI, UploadFile, File, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles

# â”€â”€ Import all the analysis functions from our analyzer module â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from analyzer import (
    load_instagram_export,
    identify_patient,
    parse_thread,
    format_for_claude,
    trim_to_token_limit,
    analyze_defense_mechanisms,
    analyze_kpis,
    qualitative_summary,
    DSM5_AVAILABLE,
)

# Import DSM-5 diagnostic function if available
if DSM5_AVAILABLE:
    from dsm5_diagnostic import analyze_dsm5_diagnosis


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  FASTAPI APP SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create the FastAPI application instance
app = FastAPI(
    title="PsychoGraph API",
    description="Instagram chat analysis with Claude AI",
    version="1.0.0"
)

# â”€â”€ CORS Middleware: allows the HTML dashboard to make API calls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Without this, browsers block cross-origin requests for security
# In production, you'd restrict allow_origins to specific domains
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],           # Allow all origins (fine for local dev)
    allow_credentials=True,        # Allow cookies/auth headers
    allow_methods=["*"],           # Allow all HTTP methods (GET, POST, etc.)
    allow_headers=["*"],           # Allow all request headers
)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ROUTE 1: Serve the dashboard HTML at the root URL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/", response_class=HTMLResponse)
def serve_dashboard():
    """
    When users visit http://127.0.0.1:8000/ in their browser,
    serve the dashboard HTML file directly.
    
    This eliminates the need to open the HTML file separately â€”
    everything runs through one URL.
    """
    dashboard_path = Path(__file__).parent / "dashboard.html"
    
    # Check if the dashboard file exists in the same folder as this script
    if not dashboard_path.exists():
        raise HTTPException(
            status_code=500,
            detail=f"dashboard.html not found. Make sure it's in {Path(__file__).parent}"
        )
    
    # Read and return the HTML content
    with open(dashboard_path, "r", encoding="utf-8") as f:
        return f.read()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ROUTE 2: Health check endpoint
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/health")
def health_check():
    """
    Simple endpoint to verify the server is running.
    Returns a JSON object with the server status.
    
    Visit http://127.0.0.1:8000/health to test.
    """
    return {"status": "ok", "message": "PsychoGraph API is running"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ROUTE 3: Main analysis endpoint
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/analyze")
async def analyze_instagram_export(file: UploadFile = File(...)):
    """
    Main API endpoint that receives an Instagram export file (or folder),
    runs the full Claude analysis pipeline, and returns JSON results.
    
    The dashboard calls this endpoint when the user uploads a file.
    
    Accepts:
      - Single message_X.json file from one conversation
      - A .zip file containing the full Instagram export folder
      
    Returns:
      JSON object matching the analysis_results.json format
    """
    
    # â”€â”€ Step 1: Validate the uploaded file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not file.filename:
        raise HTTPException(status_code=400, detail="No file provided")
    
    filename = file.filename.lower()
    
    # Accept either JSON or ZIP files
    if not (filename.endswith(".json") or filename.endswith(".zip")):
        raise HTTPException(
            status_code=400,
            detail="Please upload either a .json file or a .zip of your Instagram export"
        )
    
    # â”€â”€ Step 2: Read the uploaded file into memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        file_content = await file.read()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error reading file: {str(e)}")
    
    # â”€â”€ Step 3: Handle the file based on its type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    temp_dir = None
    try:
        if filename.endswith(".json"):
            # Single JSON file â€” parse it directly
            data = handle_json_file(file_content, filename)
            
        elif filename.endswith(".zip"):
            # ZIP archive â€” extract it to a temp folder and process
            temp_dir = tempfile.mkdtemp()
            data = handle_zip_file(file_content, temp_dir)
        
        else:
            raise HTTPException(status_code=400, detail="Unsupported file type")
        
        # â”€â”€ Step 4: Run the full analysis pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        results = run_full_analysis(data)
        
        return JSONResponse(content=results)
    
    except HTTPException:
        # Re-raise HTTP exceptions as-is (they have proper error messages)
        raise
    
    except Exception as e:
        # Catch any unexpected errors and return a 500 error
        raise HTTPException(status_code=500, detail=f"Analysis error: {str(e)}")
    
    finally:
        # â”€â”€ Step 5: Clean up temporary files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # If we created a temp directory for ZIP extraction, delete it
        if temp_dir and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HELPER FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def handle_json_file(file_content: bytes, filename: str) -> dict:
    """
    Parse a single Instagram message JSON file.
    Handles both UTF-8 and Latin-1 encodings.
    """
    # Try UTF-8 first (most common)
    try:
        text = file_content.decode("utf-8")
        data = json.loads(text)
        return {"single_file": data, "filename": filename}
    
    except UnicodeDecodeError:
        # Fall back to Latin-1 if UTF-8 fails (Instagram sometimes uses this)
        try:
            text = file_content.decode("latin-1")
            data = json.loads(text)
            return {"single_file": data, "filename": filename}
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON encoding: {str(e)}")
    
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON format: {str(e)}")


def handle_zip_file(file_content: bytes, temp_dir: str) -> dict:
    """
    Extract a ZIP file to a temporary directory and scan for Instagram
    message files. Returns a dict that can be passed to the analyzer.
    """
    import zipfile
    
    zip_path = os.path.join(temp_dir, "upload.zip")
    
    # Write the uploaded ZIP to disk temporarily
    with open(zip_path, "wb") as f:
        f.write(file_content)
    
    # Extract all files from the ZIP
    try:
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            zip_ref.extractall(temp_dir)
    except zipfile.BadZipFile:
        raise HTTPException(status_code=400, detail="Invalid ZIP file")
    
    # Return the path to the extracted folder so analyzer can scan it
    return {"folder": temp_dir}


def run_full_analysis(data: dict) -> dict:
    """
    Main analysis orchestrator. Takes either a single JSON file
    or a folder path, runs the full Claude analysis pipeline,
    and returns the results in dashboard format.
    """
    
    # â”€â”€ Case 1: Single JSON file uploaded â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "single_file" in data:
        raw_json = data["single_file"]
        filename = data.get("filename", "unknown")
        
        # Check if this is a raw Instagram message file or already-analyzed results
        if "participants" in raw_json and "messages" in raw_json:
            # This is a raw Instagram conversation file â€” analyze it
            threads = {"uploaded_conversation": raw_json}
        
        elif "patient_name" in raw_json and "conversations" in raw_json:
            # This is already an analysis_results.json file â€” just return it
            return raw_json
        
        else:
            raise HTTPException(
                status_code=400,
                detail="Unrecognized JSON format. Expected Instagram message export or analysis_results.json"
            )
    
    # â”€â”€ Case 2: ZIP folder uploaded â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    elif "folder" in data:
        folder_path = data["folder"]
        # Use the existing load_instagram_export function to scan the folder
        threads = load_instagram_export(folder_path)
    
    else:
        raise HTTPException(status_code=400, detail="Invalid data structure")
    
    # â”€â”€ Extract patient name from first thread â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not threads:
        raise HTTPException(status_code=400, detail="No Instagram message data found in upload")
    
    first_thread = next(iter(threads.values()))
    patient_name = identify_patient(first_thread.get("participants", []))
    
    # â”€â”€ Build results container â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results = {
        "patient_name": patient_name,
        "conversations": {}
    }
    
    # â”€â”€ Loop through each conversation thread and analyze it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for thread_key, thread_data in threads.items():
        
        # Extract the other person's name from participants list
        # The patient is always first, so get everyone else's names
        participants = thread_data.get("participants", [])
        other_participants = [p["name"] for p in participants if p.get("name") != patient_name]
        
        # Use the other person's name, or fall back to title or thread_key
        if other_participants:
            participant_label = ", ".join(other_participants)  # Join multiple names for group chats
        else:
            participant_label = thread_data.get("title", thread_key)
        
        # Parse the raw messages into clean format
        messages = parse_thread(thread_data, patient_name)
        
        # Skip threads where patient sent no messages
        patient_messages = [m for m in messages if m.get("is_patient", False)]
        if not patient_messages:
            continue
        
        # Format the conversation for Claude and trim if too long
        conversation_text = format_for_claude(messages)
        conversation_text = trim_to_token_limit(conversation_text)
        
        # â”€â”€ Run all three Claude analyses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            defense_data = analyze_defense_mechanisms(conversation_text, participant_label)
            kpi_data     = analyze_kpis(conversation_text, participant_label)
            summary_data = qualitative_summary(conversation_text, participant_label)
            
            # â”€â”€ Run DSM-5 diagnostic analysis if available â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            dsm5_data = None
            if DSM5_AVAILABLE:
                try:
                    dsm5_data = analyze_dsm5_diagnosis(
                        conversation_text, 
                        participant_label
                    )
                except Exception as dsm_error:
                    print(f"DSM-5 analysis failed for {participant_label}: {dsm_error}")
                    dsm5_data = {"error": str(dsm_error)}
            
            # COMPATIBILITY LAYER: Convert new two-sided format to old one-sided format
            # This allows the current dashboard to still work while we update it
            old_format_defense = {
                "defense_mechanisms": defense_data.get("patient_defense_mechanisms", {}),
                "total_defense_events": defense_data.get("patient_total", 0),
                "dominant_mechanism": defense_data.get("patient_dominant", "none")
            }
            
            old_format_kpi = {
                "kpis": kpi_data.get("patient_kpis", {}),
                "overall_health_score": kpi_data.get("patient_overall_score", 0),
                "flag_for_review": kpi_data.get("flag_for_review", False),
                "flag_reason": kpi_data.get("flag_reason", None)
            }
            
            old_format_summary = {
                "relationship_dynamic": summary_data.get("relationship_dynamic", ""),
                "behavioral_patterns": summary_data.get("patient_patterns", []),
                "red_flags": summary_data.get("patient_red_flags", []),
                "strengths": summary_data.get("patient_strengths", []),
                "therapy_suggestions": summary_data.get("therapy_suggestions", []),
                "clinical_notes": summary_data.get("clinical_notes", "")
            }
            
            # Store the results for this conversation
            results["conversations"][participant_label] = {
                "message_count":       len(messages),
                "defense_mechanisms":  old_format_defense,  # Old format for dashboard
                "kpis":                old_format_kpi,      # Old format for dashboard
                "qualitative_summary": old_format_summary,  # Old format for dashboard
                # Also save the new two-sided data for future dashboard update
                "_both_sides": {
                    "defense": defense_data,
                    "kpis": kpi_data,
                    "summary": summary_data
                },
                # Add DSM-5 diagnostic assessment
                "dsm5_diagnosis": dsm5_data if dsm5_data else {
                    "primary_diagnosis": {
                        "disorder": "DSM-5 Analysis Not Available",
                        "confidence": "Insufficient Evidence",
                        "clinical_notes": "DSM-5 diagnostic module not loaded"
                    }
                }
            }
        
        except Exception as e:
            # If one conversation fails, store the error but continue with others
            results["conversations"][participant_label] = {
                "error": str(e)
            }
    
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SERVER STARTUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    # Print startup message with instructions
    print("\n" + "="*60)
    print("ğŸ§  PsychoGraph Server Starting...")
    print("="*60)
    print("\nğŸ“ Open your browser to: http://127.0.0.1:8000")
    print("ğŸ“Š Dashboard will load automatically")
    print("ğŸ”‘ Make sure ANTHROPIC_API_KEY is set in your environment\n")
    print("Press CTRL+C to stop the server\n")
    
    # Start the Uvicorn web server
    # host="0.0.0.0" makes it accessible from other devices on your network
    # port=8000 is the default development port
    # reload=True auto-restarts the server when you edit this file (useful for dev)
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
